{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Single Layer Neural Network\n",
    "\n",
    "딥러닝 알고리즘의 가장 기본이 되는 인공신경망(artificial neural network, ANN), 그 중에서도 single-layer neural network 모델을 구현한다. \n",
    "우리는 크게 세 가지 방식으로 학습하는 법을 배운다.\n",
    "\n",
    " 1. Random Search\n",
    " 2. h-step Search\n",
    " 3. Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.11995538,  0.85299741,  0.07493303,  0.29728806,  0.57062582,\n",
       "        0.86558544,  0.46639843,  0.94477476,  0.07797726,  0.15816112])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 ~ 1 사이에 균인할 값 100 개를 생성\n",
    "x1 = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "print(x1.shape)\n",
    "x1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.2212205 ,  0.88270963,  0.40675277,  0.40526446,  0.42490813,\n",
       "        0.37767975,  0.00100176,  0.79276455,  0.853391  ,  0.42397203])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "print(x2.shape)\n",
    "x2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.24659686,  0.79725404,  0.32585629,  0.39181865,  0.48364181,\n",
       "        0.54851551,  0.24042041,  0.7798147 ,  0.55008868,  0.35943435])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 찾고자 하는 x1, x2의 계수 (0.3, 0.5), bias(0.1) 을 임의로 지정\n",
    "# x1, x2, y 값을 이용하여 임의로 정한 계수와 바이어스 값인 0.3, 0.5, 0.1 을 찾아내 보자\n",
    "y = 0.3 * x1 + 0.5 * x2 + 0.1\n",
    "\n",
    "print(y.shape)\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First idea: Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 w1 = 0.82999, w2 = 0.50185, b = 0.07364, error = 0.22775\n",
      "   8 w1 = 0.50327, w2 = 0.03789, b = 0.39384, error = 0.19886\n",
      "  11 w1 = 0.23498, w2 = 0.14065, b = 0.28255, error = 0.09172\n",
      "  30 w1 = 0.58439, w2 = 0.31754, b = 0.00198, error = 0.08732\n",
      "  47 w1 = 0.41466, w2 = 0.31027, b = 0.15500, error = 0.05519\n",
      " 165 w1 = 0.28815, w2 = 0.45759, b = 0.12090, error = 0.01107\n",
      "5436 w1 = 0.27329, w2 = 0.51953, b = 0.10879, error = 0.00868\n",
      "------------------------------------------------------------\n",
      "5436 w1 = 0.27329, w2 = 0.51953, b = 0.10879, error = 0.00868\n"
     ]
    }
   ],
   "source": [
    "# loop 회수\n",
    "num_epoch = 10000\n",
    "# 에러\n",
    "best_error = np.inf\n",
    "# bewt_w1, best_w2, best_b 값을 찾아낸 loop 회수\n",
    "best_epoch = None\n",
    "# x1 에 대한 가중치\n",
    "best_w1 = None\n",
    "# x2 에 대한 가중치\n",
    "best_w2 = None\n",
    "# bias\n",
    "best_b = None\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    # 매 반복문 마다 random 값 생성\n",
    "    w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "    b = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "    # random 하게 생성된 w1, w2, b 값을 가지고 y_predict(예측값) 계산\n",
    "    y_predict = x1 * w1 + x2 * w2 + b\n",
    "    \n",
    "    # 예측값과 실제값의 차이를 비교하여 에러 값 산출\n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    # 금번 에러가 지난번 에러값 보다 작으면 best 값 모두 업데이트\n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_epoch = epoch\n",
    "        best_w1 = w1\n",
    "        best_w2 = w2\n",
    "        best_b = b\n",
    "\n",
    "        print(\"{0:4} w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(best_epoch, best_w1, best_w2, best_b, best_error))\n",
    "\n",
    "print(\"----\" * 15)\n",
    "print(\"{0:4} w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(best_epoch, best_w1, best_w2, best_b, best_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 - h-step Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 w1 = 0.28701, w2 = 0.50724, b = 0.10101, error = 0.00388\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10000\n",
    "\n",
    "# 임의로 초기 w1, w2, b 값을 구함\n",
    "w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "b = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "h = 0.01\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    # 최초에는 초기 w1, w2, b 값을 이용하고, \n",
    "    # 그 다음부터는 업데이트 된 w1, w2, b 값을 이용하여 예측값 계산\n",
    "    y_predict = x1 * w1 + x2 * w2 + b\n",
    "    # 예측값과 실제값의 차이를 구함\n",
    "    current_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    # 에러가 0.005 보다 작으면 멈춤\n",
    "    if current_error < 0.005:\n",
    "        break\n",
    "    \n",
    "    # w1 을 h 만큼 더한 후 예측값 다시 계산\n",
    "    y_predict = x1 * (w1 + h) + x2 * w2 + b\n",
    "    \n",
    "    # w1 을 h 만큼 더한 후 계산한 예측값과 실제값의 에러 계산\n",
    "    h_plus_error = np.abs(y_predict - y).mean()\n",
    "    # 에러가 줄었으면 올바른 방향으로 w1 값을 조정했다고 보고 w1 을 w1 + h 값으로 업데이트\n",
    "    if h_plus_error < current_error:\n",
    "        w1 = w1 + h\n",
    "    else:\n",
    "        # 에러가 줄지 않았으면 찾아야 하는 값과 멀어졌다고 보고 w1 을 h 만큼 차감        \n",
    "        y_predict = x1 * (w1 - h) + x2 * w2 + b\n",
    "        # 다시 에러를 계산해서 현재 에러보다 작아졌으면 w1 을 w1 - h 값으로 업데이트\n",
    "        h_minus_error = np.abs(y_predict - y).mean()\n",
    "        if h_minus_error < current_error:\n",
    "            w1 = w1 - h\n",
    "\n",
    "    # w2 에 대해서도 w1과 동일한 방법으로 계산\n",
    "    y_predict = x1 * w1 + x2 * (w2 + h) + b\n",
    "    h_plus_error = np.abs(y_predict - y).mean()\n",
    "    if h_plus_error < current_error:\n",
    "        w2 = w2 + h\n",
    "    else:\n",
    "        y_predict = x1 * w1 + x2 * (w2 - h) + b\n",
    "        h_minus_error = np.abs(y_predict - y).mean()\n",
    "        if h_minus_error < current_error:\n",
    "            w2 = w2 - h\n",
    "    \n",
    "    # bias 에 대해서도 w1과 동일한 방법으로 계산\n",
    "    y_predict = x1 * w1 + x2 * w2 + (b + h)\n",
    "    h_plus_error = np.abs(y_predict - y).mean()\n",
    "    if h_plus_error < current_error:\n",
    "        b = b + h\n",
    "    else:\n",
    "        y_predict = x1 * w1 + x2 * w2 + (b - h)\n",
    "        h_minus_error = np.abs(y_predict - y).mean()\n",
    "        if h_minus_error < current_error:\n",
    "            b = b - h\n",
    "\n",
    "print(\"{0} w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(epoch, w1, w2, b, current_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Idea - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 w1 = -0.306487, w2 = 0.363554, b = -0.321157, error = 1.043840\n",
      " 1 w1 = 0.199660, w2 = 0.798900, b = 0.604664, error = 0.771517\n",
      " 2 w1 = -0.128827, w2 = 0.447194, b = -0.106126, error = 0.592325\n",
      " 3 w1 = 0.165268, w2 = 0.690037, b = 0.414760, error = 0.434072\n",
      " 4 w1 = -0.013207, w2 = 0.488348, b = 0.010667, error = 0.336744\n",
      " 5 w1 = 0.159583, w2 = 0.623100, b = 0.303020, error = 0.243627\n",
      " 6 w1 = 0.064615, w2 = 0.506864, b = 0.072647, error = 0.191977\n",
      " 7 w1 = 0.167662, w2 = 0.581090, b = 0.236127, error = 0.136233\n",
      " 8 w1 = 0.118841, w2 = 0.513672, b = 0.104250, error = 0.109897\n",
      " 9 w1 = 0.181503, w2 = 0.554134, b = 0.195147, error = 0.076405\n",
      "10 w1 = 0.157902, w2 = 0.514713, b = 0.119197, error = 0.064317\n",
      "11 w1 = 0.196950, w2 = 0.536443, b = 0.169290, error = 0.047458\n",
      "12 w1 = 0.186896, w2 = 0.513163, b = 0.125166, error = 0.040219\n",
      "13 w1 = 0.211951, w2 = 0.524582, b = 0.152386, error = 0.032969\n",
      "14 w1 = 0.208973, w2 = 0.510672, b = 0.126430, error = 0.028467\n",
      "15 w1 = 0.225591, w2 = 0.516478, b = 0.140883, error = 0.024917\n",
      "16 w1 = 0.226135, w2 = 0.508054, b = 0.125348, error = 0.021990\n",
      "17 w1 = 0.237552, w2 = 0.510854, b = 0.132724, error = 0.019586\n",
      "18 w1 = 0.239695, w2 = 0.505680, b = 0.123208, error = 0.017585\n",
      "19 w1 = 0.247815, w2 = 0.506911, b = 0.126701, error = 0.015733\n",
      "20 w1 = 0.250541, w2 = 0.503686, b = 0.120697, error = 0.014293\n",
      "21 w1 = 0.256507, w2 = 0.504129, b = 0.122095, error = 0.012805\n",
      "22 w1 = 0.259297, w2 = 0.502094, b = 0.118168, error = 0.011733\n",
      "23 w1 = 0.263805, w2 = 0.502167, b = 0.118467, error = 0.010529\n",
      "24 w1 = 0.266414, w2 = 0.500870, b = 0.115794, error = 0.009682\n",
      "25 w1 = 0.269901, w2 = 0.500792, b = 0.115543, error = 0.008717\n",
      "26 w1 = 0.272229, w2 = 0.499963, b = 0.113645, error = 0.008012\n",
      "27 w1 = 0.274976, w2 = 0.499842, b = 0.113147, error = 0.007251\n",
      "28 w1 = 0.276998, w2 = 0.499314, b = 0.111744, error = 0.006649\n",
      "29 w1 = 0.279193, w2 = 0.499201, b = 0.111157, error = 0.006034\n",
      "30 w1 = 0.280920, w2 = 0.498871, b = 0.110082, error = 0.005530\n",
      "31 w1 = 0.282693, w2 = 0.498786, b = 0.109491, error = 0.005027\n",
      "32 w1 = 0.284153, w2 = 0.498588, b = 0.108643, error = 0.004605\n",
      "33 w1 = 0.285597, w2 = 0.498536, b = 0.108086, error = 0.004193\n",
      "34 w1 = 0.286824, w2 = 0.498426, b = 0.107402, error = 0.003839\n",
      "35 w1 = 0.288006, w2 = 0.498404, b = 0.106898, error = 0.003501\n",
      "36 w1 = 0.289033, w2 = 0.498354, b = 0.106336, error = 0.003204\n",
      "37 w1 = 0.290005, w2 = 0.498357, b = 0.105889, error = 0.002924\n",
      "38 w1 = 0.290862, w2 = 0.498348, b = 0.105422, error = 0.002676\n",
      "39 w1 = 0.291664, w2 = 0.498370, b = 0.105031, error = 0.002446\n",
      "40 w1 = 0.292379, w2 = 0.498387, b = 0.104639, error = 0.002239\n",
      "41 w1 = 0.293043, w2 = 0.498424, b = 0.104300, error = 0.002049\n",
      "42 w1 = 0.293639, w2 = 0.498458, b = 0.103969, error = 0.001877\n",
      "43 w1 = 0.294189, w2 = 0.498504, b = 0.103676, error = 0.001720\n",
      "44 w1 = 0.294685, w2 = 0.498549, b = 0.103396, error = 0.001576\n",
      "45 w1 = 0.295142, w2 = 0.498600, b = 0.103144, error = 0.001445\n",
      "46 w1 = 0.295555, w2 = 0.498651, b = 0.102906, error = 0.001326\n",
      "47 w1 = 0.295935, w2 = 0.498705, b = 0.102690, error = 0.001217\n",
      "48 w1 = 0.296280, w2 = 0.498758, b = 0.102487, error = 0.001118\n",
      "49 w1 = 0.296596, w2 = 0.498813, b = 0.102301, error = 0.001026\n",
      "----------------------------------------\n",
      "50 w1 = 0.296596, w2 = 0.498813, b = 0.102301, error = 0.000943\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "learning_rate = 1.2\n",
    "\n",
    "w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "b = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range (num_epoch):\n",
    "    y_predict = w1 * x1 + w2 * x2 + b\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    if error < 0.001:\n",
    "        break\n",
    "    \n",
    "    \"\"\"\n",
    "    # w1, w2 를 업데이트 하는 값이 달라야 함 (같으면 맴돌이...그려보면 알수있음)\n",
    "    w1 = w1 - (y_predict - y).mean()\n",
    "    w2 = w2 - (y_predict - y).mean()\n",
    "    \"\"\"            \n",
    "    # ((y_predict - y) * x1).mean() 는 loss function 의 편미분 (공식 확인)\n",
    "    w1 = w1 - learning_rate * ((y_predict - y) * x1).mean()\n",
    "    w2 = w2 - learning_rate * ((y_predict - y) * x2).mean()\n",
    "    # bias 를 절편으로 보지않고 feature 로 인식하면 됨\n",
    "    # x3 = np.ones(100)\n",
    "    # b=  b - learning_rate * ((y_predict - y) * x3).mean()\n",
    "    b=  b - learning_rate * (y_predict - y).mean()\n",
    "    \n",
    "    print(\"{0:2} w1 = {1:.6f}, w2 = {2:.6f}, b = {3:.6f}, error = {4:.6f}\".format(epoch, w1, w2, b, error))\n",
    "\n",
    "print(\"----\" * 10)      \n",
    "print(\"{0:2} w1 = {1:.6f}, w2 = {2:.6f}, b = {3:.6f}, error = {4:.6f}\".format(epoch, w1, w2, b, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loss function 의 미분\n",
    "\n",
    "Loss function : **W 값이 정답에서 멀어질 경우 오차가 급격히 커지도록**\n",
    "\n",
    "$h(x) = wx + b$   \n",
    "\n",
    "$L(x) = \\frac{1}{2m} \\sum_{i=0}^\\infty (h(x)^i-y^i)^2$\n",
    "\n",
    "Loss function 을 미분 :\n",
    "- Loss function을 일단 간소화  \n",
    "    $\\frac{1}{2} (h(x)-y)^2 $\n",
    "\n",
    "\n",
    "- Loss function을 미분  \n",
    "    $\\frac{dL(x)}{dW} = \\frac{1}{2}2(h(x)-y) \\frac{\\partial}{\\partial W}(h(x)-y) = (h(x)-y) \\frac{\\partial}{\\partial W}(Wx-y) = (h(x)-y)x = \\frac{1}{m} \\sum_{i=0}^\\infty (h(x)^i-y^i)x^i$ \n",
    "\n",
    "\n",
    "- 미분 공식 참조\n",
    "\n",
    "    1.지수함수    \n",
    "    $f(x) = x^2, f'(x) = 2x$      \n",
    "    $f(x) = x^n, f'(x) = nx, \\frac{df(x)}{dx} = nx, f(x) = x^2 + 3x + 5, f'(x) = 2x + 3 $\n",
    "    \n",
    "    2.체인룰 (복합함수)  \n",
    "    $f(x) = g(h(x)), f'(x) = h'(x)g'(h(x))$\n",
    "    \n",
    "    3.편미분  \n",
    "    $f(x,y) = x^3 + 3y + 1,  \\frac{\\partial (x,y)}{\\partial x} = 3x, \\frac{\\partial (x,y)}{\\partial y} = 3$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
