{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_dates : 'datetime' 컬럼을 python date type 으로 처리하기 위해\n",
    "train = pd.read_csv(\"data/train.csv\", parse_dates=[\"datetime\"])\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차트를 jupyter notebook에 출력해서 보기 위한 명령어\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datatime 을 python date type 으로 load 했기 때문에 '.dt' 속성을 이용해서 년/월/일/시/분/초/요일 등의 정보에 접근 가능함\n",
    "# datatime 으로 부터 년/월/일/시/분/초 컬럼 추가\n",
    "train[\"datetime-year\"] = train[\"datetime\"].dt.year\n",
    "train[\"datetime-month\"] = train[\"datetime\"].dt.month\n",
    "train[\"datetime-day\"] = train[\"datetime\"].dt.day\n",
    "train[\"datetime-hour\"] = train[\"datetime\"].dt.hour\n",
    "train[\"datetime-minute\"] = train[\"datetime\"].dt.minute\n",
    "train[\"datetime-second\"] = train[\"datetime\"].dt.second\n",
    "\n",
    "print(train.shape)\n",
    "train[[\"datetime\", \"datetime-year\", \"datetime-month\", \"datetime-day\", \"datetime-hour\", \"datetime-minute\", \"datetime-second\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 X 3 subplot 선언\n",
    "figure, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3)\n",
    "# subplot size 선언\n",
    "figure.set_size_inches(18, 8)\n",
    "# time 관련 feature 들과 대여 수(count) 관계를 barplot 을 이용해 확인\n",
    "sns.barplot(data=train, x=\"datetime-year\", y=\"count\", ax=ax1)\n",
    "sns.barplot(data=train, x=\"datetime-month\", y=\"count\", ax=ax2)\n",
    "sns.barplot(data=train, x=\"datetime-day\", y=\"count\", ax=ax3)\n",
    "sns.barplot(data=train, x=\"datetime-hour\", y=\"count\", ax=ax4)\n",
    "sns.barplot(data=train, x=\"datetime-minute\", y=\"count\", ax=ax5)\n",
    "sns.barplot(data=train, x=\"datetime-second\", y=\"count\", ax=ax6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lesson Learned **\n",
    "  * **datetime-minute**와 **datetime-second**는 현재 기록되고 있지 않다. 그러므로 사용할 필요가 없다.\n",
    "  * train.csv와 test.csv는 **datetime-day**를 기준으로 나뉘어져 있다. 그러므로 **datetime-day**를 feature로 사용해서는 안 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore hour - workingday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 X 1 subplot 선언\n",
    "figure, (ax1, ax2) = plt.subplots(nrows=2, ncols=1)\n",
    "figure.set_size_inches(18, 8)\n",
    "\n",
    "# 시간과 대여 수(count)의 관계를 pointplot 을 이용해 확인\n",
    "sns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", ax=ax1)\n",
    "# hue=\"workingday\" 를 추가해서 주중 시간대와, 주말 시간대별 대여수 관계를 확인\n",
    "sns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", hue=\"workingday\", ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lesson Learned **\n",
    "  * 주중(workingday==1)에는 출근 시간과 퇴근 시간에 자전거를 많이 대여한다.\n",
    "  * 주말(workingday==0)에는 오후 시간에 자전거를 많이 대여한다.\n",
    "  * 주중(월,화,수,목,금)이 주말(토,일)보다 많기 때문에, 두 개를 나눠서 보지 않으면 주말의 특성을 파악할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore hour - dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요일(dayofweek) 컬럼 추가 (0 = monday, ~ 6 = sunday) \n",
    "train[\"datetime-dayofweek\"] = train[\"datetime\"].dt.dayofweek\n",
    "\n",
    "print(train.shape)\n",
    "train[[\"datetime\", \"datetime-dayofweek\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 X 1 subplot 선언\n",
    "figure, (ax1, ax2) = plt.subplots(nrows=2, ncols=1)\n",
    "figure.set_size_inches(18, 8)\n",
    "\n",
    "# 주중/주말 시간대별 대여수 차이\n",
    "sns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", hue=\"workingday\", ax=ax1)\n",
    "# 요일 시간대별 대여수 차이\n",
    "sns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", hue=\"datetime-dayofweek\", ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lesson Learned **\n",
    "  * 금요일(workingday==4)는 주중이지만, 아주 약간 주말의 특성을 반영하고 있다.\n",
    "  * 비슷하게 월요일(workingday==0)도 아주 약간 주말의 특성을 반영하고 있다.\n",
    "  * 사람들이 휴가를 월요일과 금요일에 사용하기 때문이라고 추측할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime 을 받아서 '년-월' 값을 만들어 주는 함수 정의\n",
    "def concatenate_year_month(datetime):\n",
    "    return \"{0}-{1}\".format(datetime.year, datetime.month)\n",
    "# 년-월 컬럼 추가 (년-월 별 대여량 차이 확인 하기 위해서)\n",
    "train[\"datetime-year_month\"] = train[\"datetime\"].apply(concatenate_year_month)\n",
    "\n",
    "print(train.shape)\n",
    "train[[\"datetime\", \"datetime-year_month\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1 X 1 subplot 선언\n",
    "figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "figure.set_size_inches(18, 4)\n",
    "# 연도별 대여수 차이 확인\n",
    "sns.barplot(data=train, x=\"datetime-year\", y=\"count\", ax=ax1)\n",
    "# 월별 대여수 차이 확인\n",
    "sns.barplot(data=train, x=\"datetime-month\", y=\"count\", ax=ax2)\n",
    "\n",
    "# 하나짜리 subplot 선언 (차트 하나를 원하는 사이즈로 출력하고 싶을 때 사용하는 Tip)\n",
    "figure, ax3 = plt.subplots(nrows=1, ncols=1)\n",
    "figure.set_size_inches(18, 4)\n",
    "# 위에서 새로 추가한 년-월 별 대여수 차이 확인\n",
    "sns.barplot(data=train, x=\"datetime-year_month\", y=\"count\", ax=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lesson Learned **\n",
    "  * 2011년 12월과 2012년 1월의 자전거 대여량을 비슷하지만, 두 개를 따로 놓고 보면 이를 알 수 없다.\n",
    "  * 2011년에는 8월부터 대여량이 감소하고, 2012년에는 7월부터 대여량이 감소한다. 마찬가지로 따로 놓고 보면 이를 알 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 하기위해 train 원본 데이터를 다시 load\n",
    "train = pd.read_csv(\"data/train.csv\", parse_dates=[\"datetime\"])\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터도 전처리를 같이 해야 하기 때문에 함께 load\n",
    "test = pd.read_csv(\"data/test.csv\", parse_dates=[\"datetime\"])\n",
    "\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터에 년/월/일/시간/요일 컬럼 추가\n",
    "train[\"datetime-year\"] = train[\"datetime\"].dt.year\n",
    "train[\"datetime-month\"] = train[\"datetime\"].dt.month\n",
    "train[\"datetime-day\"] = train[\"datetime\"].dt.day\n",
    "train[\"datetime-hour\"] = train[\"datetime\"].dt.hour\n",
    "train[\"datetime-dayofweek\"] = train[\"datetime\"].dt.dayofweek\n",
    "\n",
    "print(train.shape)\n",
    "train[[\"datetime\", \"datetime-year\", \"datetime-month\", \"datetime-day\", \"datetime-hour\", \"datetime-dayofweek\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터에 년/월/일/시간/요일 컬럼 추가\n",
    "test[\"datetime-year\"] = test[\"datetime\"].dt.year\n",
    "test[\"datetime-month\"] = test[\"datetime\"].dt.month\n",
    "test[\"datetime-day\"] = test[\"datetime\"].dt.day\n",
    "test[\"datetime-hour\"] = test[\"datetime\"].dt.hour\n",
    "test[\"datetime-dayofweek\"] = test[\"datetime\"].dt.dayofweek\n",
    "\n",
    "print(test.shape)\n",
    "test[[\"datetime\", \"datetime-year\", \"datetime-month\", \"datetime-day\", \"datetime-hour\", \"datetime-dayofweek\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 시킬 feature 선언\n",
    "feature_names = [\"season\", \"holiday\", \"workingday\", \"weather\",\n",
    "                 \"temp\", \"atemp\", \"humidity\", \"windspeed\",\n",
    "                 \"datetime-year\", \"datetime-hour\", \"datetime-dayofweek\"]\n",
    "\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 시킬 데이터셋(feature) 준비 : X_train\n",
    "X_train = train[feature_names]\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측(predict) 할 test 데이터셋 준비 : X_test\n",
    "X_test = test[feature_names]\n",
    "\n",
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 시킬 때 사용할 label(target, 종속변수) 컬럼 선택\n",
    "label_name = \"count\"\n",
    "\n",
    "# train 시킬 때 사용할 label(target, 종속변수) 데이터셋 준비\n",
    "y_train = train[label_name]\n",
    "\n",
    "print(y_train.shape)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log(y_train + 1)\n",
    "\n",
    "print(y_train.shape)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def rmsle(predict, actual):\n",
    "    predict = np.array(predict)\n",
    "    actual = np.array(actual)\n",
    "    \n",
    "#     log_predict = np.log(predict + 1)\n",
    "#     log_actual = np.log(actual + 1)\n",
    "    log_predict = predict + 1\n",
    "    log_actual = actual + 1\n",
    "    \n",
    "    difference = log_predict - log_actual\n",
    "    difference = np.square(difference)\n",
    "    \n",
    "    mean_difference = difference.mean()\n",
    "    \n",
    "    score = np.sqrt(mean_difference)\n",
    "    \n",
    "    return score\n",
    "\n",
    "rmsle_scorer = make_scorer(rmsle)\n",
    "rmsle_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1 - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {'apple': \"사과\", \"banana\": \"바나나\"}\n",
    "# vocabulary[\"banana\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "n_estimators = 300\n",
    "\n",
    "max_depth_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "max_features_list = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "hyperparameters_list = []\n",
    "\n",
    "for max_depth in max_depth_list:\n",
    "    for max_features in max_features_list:\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                      max_depth=max_depth,\n",
    "                                      max_features=max_features,\n",
    "                                      random_state=37,\n",
    "                                      n_jobs=-1)\n",
    "\n",
    "        score = cross_val_score(model, X_train, y_train, cv=20, \\\n",
    "                                scoring=rmsle_scorer).mean()\n",
    "\n",
    "        hyperparameters_list.append({\n",
    "            'score': score,\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'max_features': max_features,\n",
    "        })\n",
    "\n",
    "        print(\"Score = {0:.5f}\".format(score))\n",
    "\n",
    "hyperparameters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fe6951af5d02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhyperparameters_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperparameters_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mhyperparameters_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyperparameters_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperparameters_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhyperparameters_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "hyperparameters_list = pd.DataFrame.from_dict(hyperparameters_list)\n",
    "hyperparameters_list = hyperparameters_list.sort_values(by=\"score\")\n",
    "\n",
    "print(hyperparameters_list.shape)\n",
    "hyperparameters_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 - Random Search (Coarse Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "hyperparameters_list = []\n",
    "\n",
    "n_estimators = 300\n",
    "num_epoch = 100\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    max_depth = np.random.randint(low=2, high=100)\n",
    "    max_features = np.random.uniform(low=0.1, high=1.0)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                  max_depth=max_depth,\n",
    "                                  max_features=max_features,\n",
    "                                  random_state=37,\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "    score = cross_val_score(model, X_train, y_train, cv=20, \\\n",
    "                            scoring=rmsle_scorer).mean()\n",
    "\n",
    "    hyperparameters_list.append({\n",
    "        'score': score,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'max_features': max_features,\n",
    "    })\n",
    "\n",
    "    print(\"Score = {0:.5f}\".format(score))\n",
    "\n",
    "hyperparameters_list = pd.DataFrame.from_dict(hyperparameters_list)\n",
    "hyperparameters_list = hyperparameters_list.sort_values(by=\"score\")\n",
    "\n",
    "print(hyperparameters_list.shape)\n",
    "hyperparameters_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 - Random Search(Finer Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "hyperparameters_list = []\n",
    "\n",
    "n_estimators = 300\n",
    "num_epoch = 100\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    max_depth = np.random.randint(low=10, high=70)\n",
    "    max_features = np.random.uniform(low=0.4, high=1.0)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                  max_depth=max_depth,\n",
    "                                  max_features=max_features,\n",
    "                                  random_state=37,\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "    score = cross_val_score(model, X_train, y_train, cv=20, \\\n",
    "                            scoring=rmsle_scorer).mean()\n",
    "\n",
    "    hyperparameters_list.append({\n",
    "        'score': score,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'max_features': max_features,\n",
    "    })\n",
    "\n",
    "    print(\"Score = {0:.5f}\".format(score))\n",
    "\n",
    "hyperparameters_list = pd.DataFrame.from_dict(hyperparameters_list)\n",
    "hyperparameters_list = hyperparameters_list.sort_values(by=\"score\")\n",
    "\n",
    "print(hyperparameters_list.shape)\n",
    "hyperparameters_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=3000,\n",
    "                              max_depth=83,\n",
    "                              max_features=0.851358,\n",
    "                              random_state=37,\n",
    "                              n_jobs=-1)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "# def rmsle(predict, actual):\n",
    "#     predict = np.array(predict)\n",
    "#     actual = np.array(actual)\n",
    "    \n",
    "# #     log_predict = np.log(predict + 1)\n",
    "# #     log_actual = np.log(actual + 1)\n",
    "#     log_predict = predict + 1\n",
    "#     log_actual = actual + 1\n",
    "    \n",
    "#     difference = log_predict - log_actual\n",
    "#     difference = np.square(difference)\n",
    "    \n",
    "#     mean_difference = difference.mean()\n",
    "    \n",
    "#     score = np.sqrt(mean_difference)\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# rmsle_scorer = make_scorer(rmsle)\n",
    "# rmsle_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# score = cross_val_score(model, X_train, y_train, cv=20, \\\n",
    "#                         scoring=rmsle_scorer).mean()\n",
    "\n",
    "# print(\"Score = {0:.5f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "predictions = np.exp(predictions) - 1\n",
    "\n",
    "print(predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"data/bike/sampleSubmission.csv\")\n",
    "\n",
    "submission[\"count\"] = predictions\n",
    "\n",
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"baseline-script.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(feature_names, model.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
