{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand (Hyperparameter Tunning)\n",
    "https://www.kaggle.com/c/bike-sharing-demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_dates : 'datetime' 컬럼을 python date type 으로 처리하기 위해\n",
    "train = pd.read_csv(\"data/train.csv\", parse_dates=[\"datetime\"])\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차트를 jupyter notebook에 출력해서 보기 위한 명령어\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datatime 을 python date type 으로 load 했기 때문에 '.dt' 속성을 이용해서 년/월/일/시/분/초/요일 등의 정보에 접근 가능함\n",
    "# datatime 으로 부터 년/월/일/시/분/초 컬럼 추가\n",
    "train[\"datetime-year\"] = train[\"datetime\"].dt.year\n",
    "train[\"datetime-month\"] = train[\"datetime\"].dt.month\n",
    "train[\"datetime-day\"] = train[\"datetime\"].dt.day\n",
    "train[\"datetime-hour\"] = train[\"datetime\"].dt.hour\n",
    "train[\"datetime-minute\"] = train[\"datetime\"].dt.minute\n",
    "train[\"datetime-second\"] = train[\"datetime\"].dt.second\n",
    "\n",
    "print(train.shape)\n",
    "train[[\"datetime\", \"datetime-year\", \"datetime-month\", \"datetime-day\", \"datetime-hour\", \"datetime-minute\", \"datetime-second\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 X 3 subplot 선언\n",
    "figure, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3)\n",
    "# subplot size 선언\n",
    "figure.set_size_inches(18, 8)\n",
    "# time 관련 feature 들과 대여 수(count) 관계를 barplot 을 이용해 확인\n",
    "sns.barplot(data=train, x=\"datetime-year\", y=\"count\", ax=ax1)\n",
    "sns.barplot(data=train, x=\"datetime-month\", y=\"count\", ax=ax2)\n",
    "sns.barplot(data=train, x=\"datetime-day\", y=\"count\", ax=ax3)\n",
    "sns.barplot(data=train, x=\"datetime-hour\", y=\"count\", ax=ax4)\n",
    "sns.barplot(data=train, x=\"datetime-minute\", y=\"count\", ax=ax5)\n",
    "sns.barplot(data=train, x=\"datetime-second\", y=\"count\", ax=ax6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lesson Learned **\n",
    "  * **datetime-minute**와 **datetime-second**는 현재 기록되고 있지 않다. 그러므로 사용할 필요가 없다.\n",
    "  * train.csv와 test.csv는 **datetime-day**를 기준으로 나뉘어져 있다. 그러므로 **datetime-day**를 feature로 사용해서는 안 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore hour - workingday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 X 1 subplot 선언\n",
    "figure, (ax1, ax2) = plt.subplots(nrows=2, ncols=1)\n",
    "figure.set_size_inches(18, 8)\n",
    "\n",
    "# 시간과 대여 수(count)의 관계를 pointplot 을 이용해 확인\n",
    "sns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", ax=ax1)\n",
    "# hue=\"workingday\" 를 추가해서 주중 시간대와, 주말 시간대별 대여수 관계를 확인\n",
    "sns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", hue=\"workingday\", ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lesson Learned **\n",
    "  * 주중(workingday==1)에는 출근 시간과 퇴근 시간에 자전거를 많이 대여한다.\n",
    "  * 주말(workingday==0)에는 오후 시간에 자전거를 많이 대여한다.\n",
    "  * 주중(월,화,수,목,금)이 주말(토,일)보다 많기 때문에, 두 개를 나눠서 보지 않으면 주말의 특성을 파악할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore hour - dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요일(dayofweek) 컬럼 추가 (0 = monday, ~ 6 = sunday) \n",
    "train[\"datetime-dayofweek\"] = train[\"datetime\"].dt.dayofweek\n",
    "\n",
    "print(train.shape)\n",
    "train[[\"datetime\", \"datetime-dayofweek\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 X 1 subplot 선언\n",
    "figure, (ax1, ax2) = plt.subplots(nrows=2, ncols=1)\n",
    "figure.set_size_inches(18, 8)\n",
    "\n",
    "# 주중/주말 시간대별 대여수 차이\n",
    "sns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", hue=\"workingday\", ax=ax1)\n",
    "# 요일 시간대별 대여수 차이\n",
    "sns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", hue=\"datetime-dayofweek\", ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lesson Learned **\n",
    "  * 금요일(workingday==4)는 주중이지만, 아주 약간 주말의 특성을 반영하고 있다.\n",
    "  * 비슷하게 월요일(workingday==0)도 아주 약간 주말의 특성을 반영하고 있다.\n",
    "  * 사람들이 휴가를 월요일과 금요일에 사용하기 때문이라고 추측할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime 을 받아서 '년-월' 값을 만들어 주는 함수 정의\n",
    "def concatenate_year_month(datetime):\n",
    "    return \"{0}-{1}\".format(datetime.year, datetime.month)\n",
    "# 년-월 컬럼 추가 (년-월 별 대여량 차이 확인 하기 위해서)\n",
    "train[\"datetime-year_month\"] = train[\"datetime\"].apply(concatenate_year_month)\n",
    "\n",
    "print(train.shape)\n",
    "train[[\"datetime\", \"datetime-year_month\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1 X 1 subplot 선언\n",
    "figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "figure.set_size_inches(18, 4)\n",
    "# 연도별 대여수 차이 확인\n",
    "sns.barplot(data=train, x=\"datetime-year\", y=\"count\", ax=ax1)\n",
    "# 월별 대여수 차이 확인\n",
    "sns.barplot(data=train, x=\"datetime-month\", y=\"count\", ax=ax2)\n",
    "\n",
    "# 하나짜리 subplot 선언 (차트 하나를 원하는 사이즈로 출력하고 싶을 때 사용하는 Tip)\n",
    "figure, ax3 = plt.subplots(nrows=1, ncols=1)\n",
    "figure.set_size_inches(18, 4)\n",
    "# 위에서 새로 추가한 년-월 별 대여수 차이 확인\n",
    "sns.barplot(data=train, x=\"datetime-year_month\", y=\"count\", ax=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lesson Learned **\n",
    "  * 2011년 12월과 2012년 1월의 자전거 대여량을 비슷하지만, 두 개를 따로 놓고 보면 이를 알 수 없다.\n",
    "  * 2011년에는 8월부터 대여량이 감소하고, 2012년에는 7월부터 대여량이 감소한다. 마찬가지로 따로 놓고 보면 이를 알 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 하기위해 train 원본 데이터를 다시 load\n",
    "train = pd.read_csv(\"data/train.csv\", parse_dates=[\"datetime\"])\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터도 전처리를 같이 해야 하기 때문에 함께 load\n",
    "test = pd.read_csv(\"data/test.csv\", parse_dates=[\"datetime\"])\n",
    "\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터에 년/월/일/시간/요일 컬럼 추가\n",
    "train[\"datetime-year\"] = train[\"datetime\"].dt.year\n",
    "train[\"datetime-month\"] = train[\"datetime\"].dt.month\n",
    "train[\"datetime-day\"] = train[\"datetime\"].dt.day\n",
    "train[\"datetime-hour\"] = train[\"datetime\"].dt.hour\n",
    "train[\"datetime-dayofweek\"] = train[\"datetime\"].dt.dayofweek\n",
    "\n",
    "print(train.shape)\n",
    "train[[\"datetime\", \"datetime-year\", \"datetime-month\", \"datetime-day\", \"datetime-hour\", \"datetime-dayofweek\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터에 년/월/일/시간/요일 컬럼 추가\n",
    "test[\"datetime-year\"] = test[\"datetime\"].dt.year\n",
    "test[\"datetime-month\"] = test[\"datetime\"].dt.month\n",
    "test[\"datetime-day\"] = test[\"datetime\"].dt.day\n",
    "test[\"datetime-hour\"] = test[\"datetime\"].dt.hour\n",
    "test[\"datetime-dayofweek\"] = test[\"datetime\"].dt.dayofweek\n",
    "\n",
    "print(test.shape)\n",
    "test[[\"datetime\", \"datetime-year\", \"datetime-month\", \"datetime-day\", \"datetime-hour\", \"datetime-dayofweek\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 시킬 feature 선언\n",
    "feature_names = [\"season\", \"holiday\", \"workingday\", \"weather\",\n",
    "                 \"temp\", \"atemp\", \"humidity\", \"windspeed\",\n",
    "                 \"datetime-year\", \"datetime-hour\", \"datetime-dayofweek\"]\n",
    "\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 시킬 데이터셋(feature) 준비 : X_train\n",
    "X_train = train[feature_names]\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측(predict) 할 test 데이터셋 준비 : X_test\n",
    "X_test = test[feature_names]\n",
    "\n",
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 시킬 때 사용할 label(target, 종속변수) 컬럼 선택\n",
    "label_name = \"count\"\n",
    "\n",
    "# train 시킬 때 사용할 label(target, 종속변수) 데이터셋 준비\n",
    "y_train = train[label_name]\n",
    "\n",
    "print(y_train.shape)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(random_state=37)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1 - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "n_estimators = 300\n",
    "\n",
    "max_depth_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "max_features_list = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "hyperparameters_list = []\n",
    "\n",
    "for max_depth in max_depth_list:\n",
    "    for max_features in max_features_list:\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                      max_depth=max_depth,\n",
    "                                      max_features=max_features,\n",
    "                                      random_state=37,\n",
    "                                      n_jobs=-1)\n",
    "\n",
    "        score = cross_val_score(model, X_train, y_train, cv=20, \\\n",
    "                                scoring='neg_mean_absolute_error').mean()\n",
    "\n",
    "        hyperparameters_list.append({\n",
    "            'score': score,\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'max_features': max_features,\n",
    "        })\n",
    "\n",
    "        print(\"Score = {0:.5f}\".format(score))\n",
    "\n",
    "hyperparameters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_list = pd.DataFrame.from_dict(hyperparameters_list)\n",
    "hyperparameters_list = hyperparameters_list.sort_values(by=\"score\")\n",
    "\n",
    "print(hyperparameters_list.shape)\n",
    "hyperparameters_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 - Random Search (Coarse Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "hyperparameters_list = []\n",
    "\n",
    "n_estimators = 300\n",
    "num_epoch = 100\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    max_depth = np.random.randint(low=2, high=100)\n",
    "    max_features = np.random.uniform(low=0.1, high=1.0)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                  max_depth=max_depth,\n",
    "                                  max_features=max_features,\n",
    "                                  random_state=37,\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "    score = cross_val_score(model, X_train, y_train, cv=20, \\\n",
    "                            scoring='neg_mean_absolute_error').mean()\n",
    "\n",
    "    hyperparameters_list.append({\n",
    "        'score': score,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'max_features': max_features,\n",
    "    })\n",
    "\n",
    "    print(\"Score = {0:.5f}\".format(score))\n",
    "\n",
    "hyperparameters_list = pd.DataFrame.from_dict(hyperparameters_list)\n",
    "hyperparameters_list = hyperparameters_list.sort_values(by=\"score\")\n",
    "\n",
    "print(hyperparameters_list.shape)\n",
    "hyperparameters_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 - Random Search(Finer Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "hyperparameters_list = []\n",
    "\n",
    "n_estimators = 300\n",
    "num_epoch = 100\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    max_depth = np.random.randint(low=10, high=70)\n",
    "    max_features = np.random.uniform(low=0.4, high=1.0)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                  max_depth=max_depth,\n",
    "                                  max_features=max_features,\n",
    "                                  random_state=37,\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "    score = cross_val_score(model, X_train, y_train, cv=20, \\\n",
    "                            scoring='neg_mean_absolute_error').mean()\n",
    "\n",
    "    hyperparameters_list.append({\n",
    "        'score': score,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'max_features': max_features,\n",
    "    })\n",
    "\n",
    "    print(\"Score = {0:.5f}\".format(score))\n",
    "\n",
    "hyperparameters_list = pd.DataFrame.from_dict(hyperparameters_list)\n",
    "hyperparameters_list = hyperparameters_list.sort_values(by=\"score\")\n",
    "\n",
    "print(hyperparameters_list.shape)\n",
    "hyperparameters_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=3000,\n",
    "                              max_depth=83,\n",
    "                              max_features=0.851358,\n",
    "                              random_state=37,\n",
    "                              n_jobs=-1)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a : 정답, P : 측정값\n",
    "\n",
    "$|P-a|$ = mean absolute error (MAE)\n",
    "\n",
    "\n",
    "$|P-a|^2$ = mean squared error (MSE)\n",
    "\n",
    "\n",
    "$ \\sqrt{(p-a)^2}$ = root mean squared error (RMSE)\n",
    "\n",
    "\n",
    "$ \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 } $ = root mean squared log error (RMSLE)\n",
    "\n",
    "\n",
    "- 차이가 크면 클수록 페널티를 주고 싶으면 MSE 를 사용 함\n",
    "\n",
    "- 차이가 크면 클루록 페널티가 줄어들게 하고 싶으면 RMSLE 사용 함 (RMSLE 로직은 직접 구현해야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "score = cross_val_score(model, X_train, y_train, cv=20, \\\n",
    "                         scoring='neg_mean_absolute_error').mean()\n",
    "\n",
    "print(\"Score = {0:.5f}\".format(-score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "importance_by_feature = list(zip(feature_names, importances))\n",
    "importance_by_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots 를 이용해서 차트 하나의 가로/세로 사이즈를 조정하고 싶을 경우 응용\n",
    "figure, ax1 = plt.subplots(nrows=1, ncols=1)\n",
    "figure.set_size_inches(18, 4)\n",
    "\n",
    "importance_df = pd.DataFrame(importance_by_feature, columns=['feature', 'importance'])\n",
    "sns.barplot(data=importance_df, x='feature', y='importance', ax=ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "\n",
    "submission[\"count\"] = predictions\n",
    "\n",
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 파일을 구분하기 위해 파일명에 timestamp 정보 추가 하기 위한 작업 \n",
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now()\n",
    "current_date = current_date.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "description = \"bike-random-forest\"\n",
    "\n",
    "filename = \"{date}_{desc}_{score}.csv\".format(date=current_date, desc=description, score=\"{0:.5f}\".format(score))\n",
    "filepath = \"data/{filename}\".format(filename=filename)\n",
    "\n",
    "submission.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
